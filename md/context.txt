Project Overview
- The repository hosts the mdai kiosk controller (FastAPI) and React UI. RealSense D435i hardware, MediaPipe liveness, and ToF sensor integration drive the user flow.
- Controller code lives under controller/app; UI in mdai-ui; RealSense + MediaPipe live inside `controller/app/sensors`.

Liveness Pipeline (`controller/app/sensors/realsense.py`)
- Streams color (RGB), depth (Z16), and IR (Y8) at 640x480@30fps; depth aligned to color.
- Face detection + mesh run on RGB frames via MediaPipe; motion metrics derived from landmarks.
- Depth metrics: range, stdev, center-vs-outer prominence, cheek asymmetry.
- IR metrics: mean, stdev, saturation fraction, dark fraction, flicker over rolling history.
- Motion metrics: eye/mouth aspect ratio variance, nose depth change, face center shift.
- Hysteresis ensures depth_ok, ir_ok, and movement_ok accumulate into instant_alive/stable_alive.
- Returns `SimpleLivenessResult` containing RGB frame, metrics, and liveness flags.

RealSense Service (controller/app/sensors/realsense.py)
- Manages preview and liveness results broadcast; placeholder JPEG used when hardware disabled.
- Preview loop pushes native cadence frames (no artificial 1/15 throttle).
- When hardware active + MediaPipe instance present, serializes RGB frame to JPEG for preview subscribers and emits LivenessResult to queues.
- set_hardware_active(True/False) spins up / tears down MediaPipeLiveness instance in executor.

Session Manager (controller/app/session_manager.py)
- Coordinates full kiosk session: pairing_request -> qr_display -> waiting_activation -> human_detect -> stabilizing -> uploading -> waiting_ack -> complete -> idle.
- New _phase_started_at timestamp + _ensure_phase_duration(min_seconds) guarantee each phase holds ≥3s.
- _run_session steps:
  * PAIRING_REQUEST: issue token from bridge REST; min 3s before moving on.
  * QR_DISPLAY: broadcast QR payload; connect websocket to bridge; ensure 3s display before waiting for app.
  * WAITING_ACTIVATION: wait for mobile app handshake (real or debug). After handshake, enforce 3s dwell.
  * HUMAN_DETECT: activate RealSense pipeline, enforce 3s before stabilization capture.
  * STABILIZING: gather liveness results for stability_seconds (4s default). Each metric broadcast includes stability/focus/composite + instant_alive/stable_alive. After capture, ensure ≥3s before upload.
  * UPLOADING: send best frame (base64) over bridge websocket; hold phase ≥3s.
  * WAITING_ACK: wait for backend acknowledgement (timeout 120s); enforce ≥3s after ack.
  * COMPLETE: display success for ≥3s, then return to IDLE.
- Error handling sets ERROR phase and keeps it visible ≥3s before reset.
- Metrics heartbeat to UI every 30s.

Controller Endpoints (controller/app/main.py)
- /healthz: simple status.
- /debug/trigger: schedules session.
- /debug/tof-trigger: simulate ToF on/off.
- /debug/app-ready: simulate mobile app handshake.
- /preview: MJPEG stream pulling directly from RealSenseService preview queue (only active once hardware running).
- /ws/ui: websocket delivering ControllerEvent (state/backend/metrics/heartbeat).

Frontend Architecture (mdai-ui)
- React + Vite + XState state machine mirroring controller phases.
- useControllerSocket hook connects to ws://127.0.0.1:5000/ws/ui (override via env), retries, translates messages into machine events.
- StageRouter now purely phase-driven: renders IdleScreen, QRCodeStage, InstructionStage per phase without timed mock sequences.
  * human_detect -> "Center your face".
  * stabilizing -> "Hold steady".
  * uploading -> "Uploading".
  * waiting_ack -> "Processing".
  * complete -> "Completed".
  * error -> ErrorOverlay.
- PreviewSurface simplified to single iframe showing /preview when state in {human_detect, stabilizing, uploading, waiting_ack}; hidden otherwise.
- ControlPanel (sidebar) shows device info, backend/controller URLs, websocket status, heartbeats, session phase, metrics (stability/focus/composite + new instantAlive/stableAlive), logs, and debug buttons (Trigger Session, ToF Trigger).
- App.tsx: cleansed of mock helpers, tracks metrics, logs, ToF/debug states. Normalizes controller URL once for reuse. Maintains qrPayloadOverride for UI display.
- MetricsSnapshot extended with instantAlive/stableAlive booleans.

Session Flow Summary
1. Idle: camera off, preview hidden, idle animation showing.
2. Debug trigger (or real ToF) -> PAIRING_REQUEST displays "preparing session" (3s min) while issuing token.
3. QR_DISPLAY: QR code screen (min 3s) with token/payload; websocket connection to bridge opens.
4. WAITING_ACTIVATION: waits for mobile app handshake; once received, remains visible ≥3s.
5. HUMAN_DETECT: RealSense hardware activates; RGB preview stream begins; kiosk prompts "Center your face".
6. STABILIZING: controller collects liveness metrics for stability_seconds (4s); kiosk shows "Hold steady"; ensures total phase ≥3s.
7. UPLOADING: best frame (JPEG base64) sent to backend; UI shows "Uploading" at least 3s.
8. WAITING_ACK: await backend response; UI shows "Processing" for ≥3s after ack.
9. COMPLETE: "Completed" message displayed ≥3s, then return to IDLE.
10. Errors at any point switch to ERROR phase (≥3s) before cleanup.

Hardware & Mock Controls
- RealSense pipeline only runs during HUMAN_DETECT/STABILIZING/UPLOAD/WAIT phases; idle keeps camera off.
- ControlPanel debug buttons:
  * "Trigger Session" -> POST /debug/trigger (starts new session).
  * "ToF Trigger" -> POST /debug/tof-trigger { triggered: true } (simulates proximity, advancing to human_detect).
- /debug/app-ready optional to mimic mobile handshake.

Preview Behavior
- /preview returns MJPEG only when hardware active; otherwise no real frames (UI hides iframe until relevant phase).
- RealSenseService placeholder frame still available when hardware disabled via settings.

Environment & Commands
- Controller run: activate venv, export RealSense enablement, `uvicorn controller.app.main:app --host 0.0.0.0 --port 5000`.
- UI run: `npm install` (first time), `npm run dev`, open reported URL.
- With camera connected & using debug buttons, full session flow works on laptop.

Additional Notes
- Metrics broadcast includes instant_alive/stable_alive for potential future UI cues.
- Every phase broadcast updates _phase_started_at to enforce dwell via _ensure_phase_duration.
- ToF and app readiness remain edge-triggered; debug endpoints support development/testing without hardware triggers.
- Build verified via `npm run build` (tsc + Vite).

Recent UI Flow Adjustments
- Restored the original StageRouter timed sequence: idle exit -> hero hold -> "scan this" prompt before the QR stage. Processing/embedding/uploading overlays now mirror the legacy behaviour and only advance after the phase-specific timers.
- Reintroduced `processingReady` logic on the UI: once the controller reports `stable_alive` for 3 consecutive seconds, StageRouter transitions from the scan hero screens into the processing instructional panels.
- The idle TV-bars screen still exposes a "Mock ToF" button that posts to `/debug/tof-trigger`, allowing laptop testing without hardware proximity.
- The QR screen no longer includes any mock app-ready controls; advancing to capture still requires a real mobile handshake (or a manual `/debug/app-ready` POST during development).
- Compiled assets verified with `npm run build` after the StageRouter changes.
